{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import imutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate the detector\n",
    "def detect(image):\n",
    "    detector = cv2.SIFT_create()\n",
    "    #detector = cv2.SURF_create()\n",
    "    #detector = cv2.ORB_create()\n",
    "    #detector = cv2.BRISK_create()\n",
    "    \n",
    "    keypoints, descriptor = detector.detectAndCompute(image, None)\n",
    "    #print('Number of keypoints Detected:', len(keypoints))\n",
    "    #print('Shape of Descriptor:', descriptor.shape)\n",
    "    return (keypoints, descriptor)\n",
    "\n",
    "#Initiate a Brute-Force matcher\n",
    "def match(matcher_name, descriptor1, descriptor2):\n",
    "    if matcher_name == 'BF':\n",
    "        matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "        #Note that cv2.NORM_L2 is good for SIFT and SURF while cv2.NORM_HAMMING is good for ORB and BRISK\n",
    "    elif matcher_name =='FLANN':\n",
    "        FLANN_INDEX_KDTREE = 1  \n",
    "        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)  \n",
    "        search_params = dict(checks=50)  \n",
    "    \n",
    "        matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    else:\n",
    "        print(\"No such matcher:\", matcher_name)\n",
    "        \n",
    "    #Match descriptors--knnMatch or match\n",
    "    matches = matcher.knnMatch(descriptor1, descriptor2, 2)\n",
    "    #matches = matcher.match(descriptors1, descriptors2) #parameter 'crossCheck' can be True\n",
    "    \n",
    "    #Apply ratio test\n",
    "    good_matches = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.75 * n.distance: #0.75 is ratio_thres\n",
    "            good_matches.append(m)\n",
    "        \n",
    "    #Sort them in the order of their distance\n",
    "    good_matches = sorted(good_matches, key = lambda x:x.distance)\n",
    "    \n",
    "    return good_matches\n",
    "\n",
    "#Find the homography matrix and transform the image\n",
    "def transform(image1, image2, keypoints1, keypoints2, matches):\n",
    "    #Obtain the coordinates of the matched keypoints\n",
    "    pos1 = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1,1,2)  \n",
    "    pos2 = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1,1,2)  \n",
    "    \n",
    "    #Find the homography matrix\n",
    "    M, _ = cv2.findHomography(pos1, pos2, cv2.RANSAC, 4)\n",
    "    \n",
    "    #Transform the image by the homography matrix\n",
    "    result = cv2.warpPerspective(image1, M, (image1.shape[1] + image2.shape[1], image1.shape[0] + image2.shape[0]))\n",
    "\n",
    "    return result\n",
    "\n",
    "#Remove the black border\n",
    "def cut(image, flag):\n",
    "    rows, cols = np.where(image[:, :, 0] != 0)\n",
    "    min_row, max_row = min(rows), max(rows) + 1\n",
    "    min_col, max_col = min(cols), max(cols) + 1\n",
    "    image = image[min_row:max_row, min_col:max_col, :]\n",
    "    \n",
    "    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _,thresh = cv2.threshold(img_gray, 0, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    if flag:\n",
    "        \n",
    "        top = bottom = 0\n",
    "        left = right = 0\n",
    "\n",
    "        for row in range(thresh.shape[0]):\n",
    "            if thresh[row, 0] != 0:\n",
    "                break  \n",
    "        top = row\n",
    "        \n",
    "        for row in range(thresh.shape[0]-1, 0, -1):\n",
    "            if thresh[row, 0] != 0:\n",
    "                break\n",
    "        bottom = row\n",
    "\n",
    "        for col in range(thresh.shape[1]):\n",
    "            if thresh[0, col] != 0:\n",
    "                break\n",
    "        left = col\n",
    "        \n",
    "        for col in range(thresh.shape[1]-1, 0, -1):\n",
    "            if thresh[0, col] != 0:\n",
    "                break\n",
    "        right = col\n",
    "\n",
    "        image = image[top:bottom, left:right, :]\n",
    "        \n",
    "    return image\n",
    "\n",
    "def stitch(image1, image2, flag):\n",
    "    image1_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    image2_gray = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Detect the keypoints\n",
    "    keypoints1, descriptor1 = detect(image1_gray)\n",
    "    keypoints2, descriptor2 = detect(image2_gray)\n",
    "\n",
    "    #Match the keypoints\n",
    "    matches = match(matcher_name, descriptor1, descriptor2)\n",
    "    if len(matches) < 30:\n",
    "        print(\"The num of matched keypoints is too small\")\n",
    "        return None\n",
    "    \n",
    "    #Draw matches and save\n",
    "    match_map = cv2.drawMatches(img1 = image1,\n",
    "                                keypoints1 = keypoints1,\n",
    "                                img2 = image2,\n",
    "                                keypoints2 = keypoints2,\n",
    "                                matches1to2 = matches[:100],\n",
    "                                outImg = None,\n",
    "                                flags = cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    cv2.imwrite(os.path.join(result_path, 'match_map.jpg'), match_map)\n",
    "\n",
    "    result = transform(image1, image2, keypoints1, keypoints2, matches)\n",
    "\n",
    "    result_cut = cut(result, False)\n",
    "\n",
    "    #Check the relative position and swap them if it is incorrect\n",
    "    if np.size(result) < np.size(image1) * 0.95:\n",
    "        matches = match(matcher_name, descriptor2, descriptor1)\n",
    "        if len(matches) < 30:\n",
    "            print(\"The num of matched keypoints is too small\")\n",
    "            return None\n",
    "\n",
    "        result = transform(image2, image1, keypoints2, keypoints1, matches)\n",
    "        \n",
    "    result[0:image2.shape[0], 0:image2.shape[1]] = np.maximum(image2, result[0:image2.shape[0], 0:image2.shape[1]])\n",
    "\n",
    "    result = cut(result, flag)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image number: 3\n"
     ]
    }
   ],
   "source": [
    "matcher_name = 'BF'\n",
    "img_path = 'figures/test4/'\n",
    "img_list = [img_name for img_name in os.listdir(img_path) if img_name.endswith('.jpg') or img_name.endswith('.png')] \n",
    "img_num = len(img_list)\n",
    "print(\"image number:\", img_num)\n",
    "assert img_num >= 2, \"There must be at least two images\"\n",
    "\n",
    "result_path = 'result/test4/'\n",
    "if not os.path.exists(result_path):\n",
    "    os.mkdir(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if img_num == 2:\n",
    "    #Read the input images\n",
    "    image1 = cv2.imread(os.path.join(img_path, img_list[0]))\n",
    "    \n",
    "    image2 = cv2.imread(os.path.join(img_path, img_list[1]))\n",
    "\n",
    "    result = stitch(image1, image2, True)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(result_path, 'stitching_image.jpg'), result)\n",
    "    \n",
    "    print(\"Successfully stitch two images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stitch 3 images\n"
     ]
    }
   ],
   "source": [
    "if img_num > 2:\n",
    "    flags = np.full(img_num, False)\n",
    "\n",
    "    image1 = cv2.imread(os.path.join(img_path, img_list[0]))\n",
    "\n",
    "    flags[0] = True\n",
    "\n",
    "    for i in range(1, img_num):\n",
    "        for j in range(0, img_num):\n",
    "            if flags[j] == True:\n",
    "                continue\n",
    "            image2 = cv2.imread(os.path.join(img_path, img_list[j]))\n",
    "\n",
    "            result = stitch(image1, image2, False)\n",
    "            \n",
    "            if not result is None:\n",
    "                flags[j] = True\n",
    "                \n",
    "                image1 = result\n",
    "    for flag in flags:\n",
    "        if flag == False:\n",
    "            print(\"These images can't  be stitched to one image\")\n",
    "            exit()\n",
    "    \n",
    "    cv2.imwrite(os.path.join(result_path, 'stitching_image.jpg'), result)\n",
    "    \n",
    "    print(\"Successfully stitch {} images\".format(img_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
